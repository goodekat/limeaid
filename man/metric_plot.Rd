% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/plot-metric_plot.R
\name{metric_plot}
\alias{metric_plot}
\title{Plot of LIME Comparison Metrics}
\usage{
metric_plot(explanations, metrics = "all")
}
\arguments{
\item{explanations}{Explain dataframe from the list returned by apply_lime.}

\item{metrics}{Vector specifying metrics to compute. Default is 'all'. See details for metrics available.}
}
\description{
Plots the specified comparison metrics versus LIME tunning parameters.
}
\details{
The metrics available are listed below.

\itemize{
  \item \code{ave_r2}: Average explainer model R2 value computed over all explanations in the test set.
  \item \code{msee}: Mean square explanation error computed over all explanations in the test set.
  \item \code{ave_fidelity}: Average fidelity metric (Ribeiro et. al. 2016) computed over all explanations in the test set.
}
}
\examples{

# Prepare training and testing data
x_train = sine_data_train[c("x1", "x2", "x3")]
y_train = factor(sine_data_train$y)
x_test = sine_data_test[1:5, c("x1", "x2", "x3")]

# Fit a random forest model
rf <- randomForest::randomForest(x = x_train, y = y_train) 

# Run apply_lime
res <- apply_lime(train = x_train, 
                  test = x_test, 
                  model = rf,
                  label = "1",
                  n_features = 2,
                  sim_method = 'quantile_bins',
                  nbins = 2:3, 
                  gower_pow = c(1, 5))

# Plot metrics to compare LIME implementations
metric_plot(res$explain)

# Return a plot with only the MSEE values
metric_plot(res$explain, metrics = "msee")
}
\references{
Ribeiro, M. T., S. Singh, and C. Guestrin, 2016:
  "why should I trust you?": Explaining the predictions of any classifier.
  Proceedings of the 22nd ACM SIGKDD Inter- national Conference on
  Knowledge Discovery and Data Mining, San Francisco, CA, USA, August
  13-17, 2016, 1135â€“1144.
}
