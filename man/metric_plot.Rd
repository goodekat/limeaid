% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/plot-metric_plot.R
\name{metric_plot}
\alias{metric_plot}
\title{Plot of LIME Comparison Metrics}
\usage{
metric_plot(explanations, metrics = "all")
}
\arguments{
\item{explanations}{Explain dataframe from the list returned by apply_lime.}

\item{metrics}{Vector specifying metrics to compute. Default is 'all'. See details for metrics available.}
}
\description{
Plots the specified comparison metrics versus LIME tunning parameters.
}
\details{
The metrics available are listed below.

\itemize{
  \item \code{ave_r2}: Average explainer model R2 value computed over all explanations in the test set.
  \item \code{msee}: Mean square explanation error computed over all explanations in the test set.
  \item \code{ave_fidelity}: Average fidelity metric (Ribeiro et. al. 2016) computed over all explanations in the test set.
}
}
\examples{

# Create Random Forest model on the sine data
rfsine <- caret::train(x = sine_data_train[c("x1", "x2", "x3")],
                       y = sine_data_train$y,
                       method = "rf")

# Apply lime several implementations of LIME
sine_lime_explain <-
   apply_lime(train = sine_data_train[c("x1", "x2", "x3")],
              test = sine_data_test[c("x1", "x2", "x3")],
              model = rfsine,
              label = "1",
              n_features = 2,
              sim_method = c('quantile_bins', 'kernel_density'),
              nbins = c(3, 4),
              seed = 20190914)

# Plot metrics to compare LIME implementations
metric_plot(sine_lime_explain$explain)

# Return a plot with only the MSEE values
metric_plot(sine_lime_explain$explain, metrics = "msee")
}
\references{
Ribeiro, M. T., S. Singh, and C. Guestrin, 2016:
  "why should I trust you?": Explaining the predictions of any classifier.
  Proceedings of the 22nd ACM SIGKDD Inter- national Conference on
  Knowledge Discovery and Data Mining, San Francisco, CA, USA, August
  13-17, 2016, 1135â€“1144.
}
